{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c78e367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a7e3b",
   "metadata": {},
   "source": [
    "# Q Learner Class \n",
    "\n",
    "#### Predict/Fit/Dump/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93df2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \n",
    "    \"\"\"\n",
    "    Q Learner Object\n",
    "    \n",
    "    :param num_states: The number of states to consider.\n",
    "    :param num_actions: The number of actions available..\n",
    "    :param alpha: The learning rate used in the update rule. Should range between 0.0 and 1.0 with 0.2 as a typical value.\n",
    "    :param gamma: The discount rate used in the update rule. Should range between 0.0 and 1.0 with 0.9 as a typical value.\n",
    "    \n",
    "    :param random_action_rate: Random action rate: the probability of selecting a random action at each step. \n",
    "     (Should range between 0.0 (no random actions) to 1.0 (always random action) with 0.5 as a typical value.)\n",
    "     \n",
    "    :param random_action_decay_rate: Random action decay rate, after each update, rar = rar * radr. \n",
    "     (Ranges between 0.0 (immediate decay to 0) and 1.0 (no decay). Typically 0.99.)\n",
    "     \n",
    "    :param verbose: If “verbose” is True, your code can print out information for debugging.    \n",
    "    \n",
    "    \"\"\"\n",
    "    SERIALIZED_LEARNER_STATE_FILE = \"Q.pickle\"\n",
    "    \n",
    "    def __init__(        \n",
    "        self,\n",
    "        *,\n",
    "        num_states: int = 100,\n",
    "        num_actions: int = 4,\n",
    "        alpha: float = 0.000001,\n",
    "        gamma: float = 0.9,\n",
    "        random_action_rate: float = 0.99,\n",
    "        random_action_decay_rate: float = 0.99,\n",
    "        verbose: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor method\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.random_action_rate = random_action_rate\n",
    "        self.random_action_decay_rate = random_action_decay_rate\n",
    "        self.current_state = 0\n",
    "        self.current_action = 0\n",
    "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
    "        print(\"Q Learner Class Initiated\")\n",
    "\n",
    "    def predict(self, new_state: int) -> int:\n",
    "        \n",
    "        \"\"\"\n",
    "        Updates state without any changes to Q table \n",
    "        \n",
    "        :param new_state: The new state \n",
    "        :return: Selected action along with the random flag\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.current_state = new_state\n",
    "        \n",
    "        if rand.random() < self.random_action_rate:\n",
    "            random_flag = True\n",
    "            action = int(rand.randint(0, self.num_actions - 1))\n",
    "        else:\n",
    "            random_flag = False\n",
    "            action = int(np.argmax(self.q_values[self.current_state]))\n",
    "                       \n",
    "        self.current_action = action\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"state = {new_state}, action = {action}\")\n",
    "        return action, random_flag\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self, s_prime: int, immediate_reward: float) -> int:\n",
    "        \n",
    "        \"\"\"\n",
    "        Updates the Q table and returns an action\n",
    "\n",
    "        :param s_prime: The new state\n",
    "        :param immediate_reward: The immediate reward\n",
    "        :return: The selected action along with random flag\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        impr_est = immediate_reward + (\n",
    "            self.gamma\n",
    "            * self.q_values[s_prime, np.argmax(self.q_values[s_prime])]\n",
    "        )\n",
    "        self.q_values[self.current_state, self.current_action] = (\n",
    "            1 - self.alpha\n",
    "        ) * self.q_values[self.current_state, self.current_action] + (\n",
    "            self.alpha * impr_est\n",
    "        )\n",
    "\n",
    "        if rand.random() < self.random_action_rate:\n",
    "            random_flag = True\n",
    "            action = int(rand.randint(0, self.num_actions - 1))\n",
    "        else:\n",
    "            random_flag = False\n",
    "            action = int(np.argmax(self.q_values[s_prime]))\n",
    "\n",
    "        self.random_action_rate = (\n",
    "            self.random_action_rate * self.random_action_decay_rate\n",
    "        )\n",
    "        self.current_state = s_prime\n",
    "        self.current_action = action\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"s = {s_prime}, a = {action}, r={immediate_reward}\")\n",
    "        return action, random_flag\n",
    "\n",
    "    def dump(self):\n",
    "        # Saving the current state of the learner to be later used for testing\n",
    "        with open(\n",
    "            self.SERIALIZED_LEARNER_STATE_FILE, \"wb\"\n",
    "        ) as serialized_learner_state_write:\n",
    "            pickle.dump(\n",
    "                self,\n",
    "                serialized_learner_state_write,\n",
    "                protocol=pickle.DEFAULT_PROTOCOL,\n",
    "            )\n",
    "\n",
    "    def load(filepath) -> \"QLearner\":\n",
    "        # Rerieving the state of the learner persisted during training\n",
    "        with open(filepath, \"rb\") as serialized_learner_state_read:\n",
    "            return pickle.load(serialized_learner_state_read)\n",
    "        \n",
    "   # def __str__(self):\n",
    "    #    return \"<\"+str(self.<<x>>)+\", \"+str(self.<<y>>)+\">\"\n",
    "    \n",
    "\n",
    " #   def __eq__(self, obj):\n",
    " #       if isinstance(obj, QLearner):\n",
    " #           return (\n",
    " #               obj.num_states == self.num_states\n",
    " #               and obj.num_actions == self.num_actions\n",
    " #               and obj.random_action_rate == self.random_action_rate\n",
    " #               and obj.random_action_decay_rate == self.random_action_decay_rate\n",
    " #               and np.array_equal(obj.q_values, self.q_values)\n",
    " #           )\n",
    " #       return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094df9e",
   "metadata": {},
   "source": [
    "# Maze game functions\n",
    "\n",
    "#### render/ goal/ cliff, out of bounds, step/discretize, create maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e2b2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmap(data: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Prints out the map\n",
    "\n",
    "    :param data: 2D array that stores the map\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"-----------\")\n",
    "    map_tile = {\n",
    "        0: \" \",  # Empty space\n",
    "        9: \".\",  # Trail\n",
    "        1: \"*\",  # Current location of Robot\n",
    "        2: \"X\",  # Goal\n",
    "        3: \"Y\",  # Cliff\n",
    "    }\n",
    "    for row in range(0, data.shape[0]):\n",
    "        for col in range(0, data.shape[1]):\n",
    "            print(map_tile[data[row, col]], end=\" \")\n",
    "        print()\n",
    "    print(\"-----------\")\n",
    "\n",
    "\n",
    "def get_goal_location(data: np.ndarray) -> Tuple:\n",
    "    \"\"\"\n",
    "    finds the goal location in the maze\n",
    "    \"\"\"\n",
    "    goal_x, goal_y = np.where(data == 3)\n",
    "    return goal_x, goal_y\n",
    "\n",
    "\n",
    "def get_cliff_locations(data: np.ndarray) -> List:\n",
    "    \n",
    "    \"\"\" Returns the locations of the cliffs \"\"\"\n",
    "    \n",
    "    shp = data.shape\n",
    "    location = []\n",
    "    for i in range(0, shp[0]):\n",
    "        for j in range(shp[1]):\n",
    "            if data[i, j] == 2:\n",
    "                location.append([i, j])\n",
    "    return location\n",
    "\n",
    "\n",
    "def check_pos_valid(data: np.ndarray, new_x: int, new_y: int) -> bool:\n",
    "    \n",
    "    \"\"\" Checks if the new position is valid \"\"\"\n",
    "    \n",
    "    shp = data.shape\n",
    "    \n",
    "    if (new_x < 0) | (new_x > shp[0] - 1):\n",
    "        return False\n",
    "    if (new_y < 0) | (new_y > shp[1] - 1):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def impl_action(\n",
    "    data: np.ndarray, curr_x: int, curr_y: int, action: int, cliffs: List\n",
    "):\n",
    "    new_x = curr_x\n",
    "    new_y = curr_y\n",
    "    if action == 0:  # left\n",
    "        new_y = curr_y - 1\n",
    "    elif action == 1:  # right\n",
    "        new_y = curr_y + 1\n",
    "    elif action == 2:  # up\n",
    "        new_x = curr_x - 1\n",
    "    else:  # down\n",
    "        new_x = curr_x + 1\n",
    "\n",
    "    rewards = -1  # default reward\n",
    "    if check_pos_valid(data, new_x, new_y):\n",
    "        if [new_x, new_y] in cliffs:\n",
    "            rewards = -100  # penalty for hitting a cliff\n",
    "        data[curr_x, curr_y] = 9\n",
    "        data[new_x, new_y] = 1\n",
    "        return data, rewards, new_x, new_y\n",
    "    rewards = -10  # penalty for going off the map\n",
    "    return data, rewards, curr_x, curr_y\n",
    "\n",
    "\n",
    "def discretize(curr_x: int, curr_y: int):\n",
    "    return curr_x * 10 + curr_y\n",
    "\n",
    "\n",
    "\n",
    "def create_maze():\n",
    "    \"\"\"\n",
    "    loading the maze from csv file and testing the algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = \"maze.csv\"\n",
    "    with open(file_path, encoding=\"utf8\") as maze_file:\n",
    "        maze = np.array(\n",
    "            [\n",
    "                list(map(float, s.strip().split(\",\")))\n",
    "                for s in maze_file.readlines()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return maze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11128d",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "#### Added params and starting states to train each state on multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8fa74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(start_positions, maze):\n",
    "    \"\"\"\n",
    "    loading the maze from csv file and training the algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # Qlearning training process\n",
    "    num_states = discretize(maze.shape[0] - 1, maze.shape[1])\n",
    "    \n",
    "    learner = QLearner(\n",
    "        num_states=num_states,\n",
    "        num_actions=4,\n",
    "        alpha=0.2,\n",
    "        gamma=0.9,\n",
    "        random_action_rate=0.98,\n",
    "        random_action_decay_rate=0.99,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    cliffs = get_cliff_locations(maze)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    goal_x, goal_y = get_goal_location(maze)\n",
    "    print(\"Goal Position:\", goal_x, \"x\",goal_y)\n",
    "    print(\" \")\n",
    "    #start_pos1, start_pos2 = np.where(maze == 1)\n",
    "    #print(\"Starting position is:\", start_pos1, start_pos2)\n",
    "    \n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"Training a \",maze.shape[0],\"x\",maze.shape[1],\"maze grid;\")\n",
    "    #print(\"**********************************\")\n",
    "    \n",
    "    #print(maze)\n",
    "    #printmap(maze)\n",
    "\n",
    "    for start_x, start_y in start_positions:\n",
    "        x,y = np.where(maze==1)\n",
    "        maze[x,y] = 0\n",
    "        maze[start_x,start_y]=1\n",
    "        printmap(maze)\n",
    "        \n",
    "        for _ in trange(1, 1000, unit=\"episode\"):  \n",
    "            \n",
    "            maze1 = maze.copy()\n",
    "            done = False\n",
    "            curr_x, curr_y = np.where(maze1 == 1)\n",
    "            action, random_flag = learner.predict(discretize(curr_x, curr_y))\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                # \n",
    "                curr_x, curr_y = np.where(maze1 == 1)\n",
    "                maze1, rewards, curr_x, curr_y = impl_action(maze1, curr_x, curr_y, action, cliffs)\n",
    "                s_prime = discretize(curr_x, curr_y)\n",
    "                action, random_flag = learner.fit(s_prime, rewards)\n",
    "\n",
    "                if [curr_x, curr_y] in cliffs or (curr_x == goal_x) & (curr_y == goal_y):\n",
    "                    done = True\n",
    "    learner.dump()                \n",
    "    print(\"Training Complete\")\n",
    "    return goal_x, goal_y, learner\n",
    "    print(\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5245e",
   "metadata": {},
   "source": [
    "# Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0752d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(start_positions, goal_x, goal_y, maze, learner):\n",
    "    #start_positions = [(1,0),(1,4),(3,3),(5,2),(6,0),(6,4),(8,2),(9,0),(9,4)]\n",
    "    for start_x, start_y in start_positions:\n",
    "        x,y = np.where(maze==1)\n",
    "        maze[x,y] = 0\n",
    "        maze[start_x,start_y]=1\n",
    "    \n",
    "        #print(QLearner.SERIALIZED_LEARNER_STATE_FILE)\n",
    "        print(\"-------------------------\")\n",
    "        #learner = QLearner.load(QLearner.SERIALIZED_LEARNER_STATE_FILE)\n",
    "        print(\" \")\n",
    "        print(\"Testing Initiated: Maze Game\")\n",
    "        print(\" \")\n",
    "        \n",
    "        \n",
    "        done = False\n",
    "        maze2 = maze.copy()   \n",
    "        print(\"..............................\")\n",
    "        print(\"Testing a \",maze2.shape[0],\"x\",maze2.shape[1],\"maze grid;\")\n",
    "        print(\"..............................\")\n",
    "        print(\" \")\n",
    "        print(maze2)\n",
    "        print(\" \")\n",
    "        print(\"Maze Starting Position: \", start_x, start_y)\n",
    "        printmap(maze2)\n",
    "        #goal_x, goal_y = np.where(maze2 == 3)\n",
    "    \n",
    "    \n",
    "        while not done:\n",
    "            old_x, old_y = np.where(maze2 == 1)\n",
    "            action, random_flag = learner.predict(discretize(old_x, old_y))\n",
    "            curr_x, curr_y = old_x, old_y\n",
    "            #print(\"Maze Trajectory\")\n",
    "            if action == 0:  # left\n",
    "                curr_y = old_y - 1\n",
    "            elif action == 1:  # right\n",
    "                curr_y = old_y + 1\n",
    "            elif action == 2:  # up\n",
    "                curr_x = old_x - 1\n",
    "            else:  # down\n",
    "                curr_x = old_x + 1\n",
    "            maze2[old_x, old_y] = 9\n",
    "            maze2[curr_x, curr_y] = 1\n",
    "            old_x, old_y = curr_x, curr_y\n",
    "            print(\"Path Sequence: \")\n",
    "            printmap(maze2)\n",
    "            if (curr_x == goal_x) & (curr_y == goal_y):\n",
    "                done = True\n",
    "        print(\" \")\n",
    "        print(\"<<<<<<<< Final path chosen:<<<<<<<<<<<<<\")\n",
    "        printmap(maze2)\n",
    "        assert done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f469f0",
   "metadata": {},
   "source": [
    "# Render Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31e29203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze Chosen: \n",
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X     X \n",
      "    X     \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X   * \n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "maze = create_maze()\n",
    "start_positions = [\n",
    "    #(1, 0),\n",
    "    (1, 0),\n",
    "    (1, 4),\n",
    "    (3, 3),\n",
    "    (5, 2),\n",
    "    (6, 0),\n",
    "    (6, 4),\n",
    "    (8, 2),\n",
    "    (9, 0),\n",
    "    (9, 4),\n",
    "]\n",
    "print(\"Maze Chosen: \")\n",
    "printmap(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22e12d",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "#### Different starting states trained on multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30eb517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learner Class Initiated\n",
      " \n",
      " \n",
      "Goal Position: [0] x [2]\n",
      " \n",
      " \n",
      "Training a  10 x 5 maze grid;\n",
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X     X \n",
      "    X     \n",
      "X       X \n",
      "  X * X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 1793.54episode/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X     X \n",
      "    X     \n",
      "X       X \n",
      "  X   X   \n",
      "*         \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 1573.36episode/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X     X \n",
      "    X     \n",
      "X       X \n",
      "  X   X   \n",
      "        * \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 1571.58episode/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X     X \n",
      "    X     \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "    *   X \n",
      "  X X     \n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 1038.63episode/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X     X \n",
      "    X     \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "* X X     \n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 1006.36episode/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X     X \n",
      "    X     \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X   * \n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:01<00:00, 693.89episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_positions = [(5, 2), (6, 0), (6, 4), (8, 2), (9, 0), (9, 4)]\n",
    "goal_x, goal_y, learner = train(start_positions, maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16eea15",
   "metadata": {},
   "source": [
    "# ----------------------- ******---------------------- # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e5248",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6b7180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      " \n",
      "Testing Initiated: Maze Game\n",
      " \n",
      "..............................\n",
      "Testing a  10 x 5 maze grid;\n",
      "..............................\n",
      " \n",
      "[[0. 0. 3. 0. 0.]\n",
      " [0. 0. 0. 2. 0.]\n",
      " [0. 2. 0. 0. 2.]\n",
      " [0. 0. 2. 1. 0.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [0. 2. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 2. 2. 2. 0.]\n",
      " [0. 0. 0. 0. 2.]\n",
      " [0. 2. 2. 0. 0.]]\n",
      " \n",
      "Maze Starting Position:  3 3\n",
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X     X \n",
      "    X *   \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n",
      "Path Sequence: \n",
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X   * X \n",
      "    X .   \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n",
      "Path Sequence: \n",
      "-----------\n",
      "    Y     \n",
      "      X   \n",
      "  X * . X \n",
      "    X .   \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n",
      "Path Sequence: \n",
      "-----------\n",
      "    Y     \n",
      "    * X   \n",
      "  X . . X \n",
      "    X .   \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n",
      "Path Sequence: \n",
      "-----------\n",
      "    *     \n",
      "    . X   \n",
      "  X . . X \n",
      "    X .   \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n",
      " \n",
      "<<<<<<<< Final path chosen:<<<<<<<<<<<<<\n",
      "-----------\n",
      "    *     \n",
      "    . X   \n",
      "  X . . X \n",
      "    X .   \n",
      "X       X \n",
      "  X   X   \n",
      "          \n",
      "  X X X   \n",
      "        X \n",
      "  X X     \n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Test maze game\n",
    "start_positions = [(3,3)]\n",
    "test(start_positions, goal_x, goal_y, maze, learner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
